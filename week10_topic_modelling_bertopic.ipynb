{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Alameer/NLP/blob/main/week10_topic_modelling_bertopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function that returns the required local i.e. UTF-8\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAJ5B6qXDmiZ"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic\n",
        "!pip install flair\n",
        "!apt-get -qq install -y libfluidsynth1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kK39kSwEIgg"
      },
      "outputs": [],
      "source": [
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Text preprocessiong\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "# Topic model\n",
        "from bertopic import BERTopic\n",
        "# Dimension reduction\n",
        "from umap import UMAP\n",
        "# Clustering\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "# Count vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Sentence transformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# Flair\n",
        "from transformers.pipelines import pipeline\n",
        "from flair.embeddings import TransformerDocumentEmbeddings, WordEmbeddings, DocumentPoolEmbeddings, StackedEmbeddings"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8FGdQrrEz2e"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "import urllib.request\n",
        "    \n",
        "url = urllib.request.urlopen(\"https://github.com/Ali-Alameer/NLP/raw/main/data/NIPS%20Papers.zip\")\n",
        "\n",
        "with ZipFile(BytesIO(url.read())) as my_zip_file:\n",
        "    temp = my_zip_file.open('NIPS Papers/papers.csv')\n",
        "\n",
        "papers_nips = pd.read_csv(temp)\n",
        "# to minimise compute \n",
        "papers_nips = papers_nips.iloc[0:1000]\n",
        "# Print head\n",
        "papers_nips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjyuzbcKFza-"
      },
      "outputs": [],
      "source": [
        "# Get the dataset information\n",
        "papers_nips.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQuEyDa0GEXU"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(f'There are {len(stopwords)} default stopwords. They are {stopwords}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYKJAkufGHDj"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "papers_nips['abstract_without_stopwords'] = papers_nips['paper_text'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))\n",
        "# Lemmatization\n",
        "papers_nips['abstract_lemmatized'] = papers_nips['abstract_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))\n",
        "# Take a look at the data\n",
        "papers_nips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOzBzoibG71n"
      },
      "outputs": [],
      "source": [
        "# Initiate UMAP\n",
        "umap_model = UMAP(n_neighbors=15, \n",
        "                  n_components=5, \n",
        "                  min_dist=0.0, \n",
        "                  metric='cosine', \n",
        "                  random_state=100)\n",
        "# Clustering model\n",
        "# hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples = 5, \n",
        "# metric='euclidean', prediction_data=True)\n",
        "kmeans_model = KMeans(n_clusters=9)\n",
        "# Initiate a sentence transformer model\n",
        "sentence_model = SentenceTransformer(\"paraphrase-albert-small-v2\")\n",
        "# Initiate a pretrained model\n",
        "hf_model = pipeline(\"feature-extraction\", model=\"distilroberta-base\")\n",
        "\n",
        "# Initiate a pretrained embedding model\n",
        "roberta_model = TransformerDocumentEmbeddings('roberta-base')\n",
        "# Initiate another pretrained embedding model\n",
        "glove_embedding = WordEmbeddings('crawl')\n",
        "document_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])\n",
        "# Stack the two pretrained embedding models\n",
        "stacked_embeddings = StackedEmbeddings(embeddings=[roberta_model, \n",
        "document_glove_embeddings])\n",
        "\n",
        "# Count vectorizer\n",
        "vectorizer_model = CountVectorizer(min_df=10)\n",
        "\n",
        "# Initiate BERTopic\n",
        "# topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True,hdbscan_model=kmeans_model,\n",
        "#                        embedding_model=stacked_embeddings,min_topic_size=5, n_gram_range=(1, 3),diversity=0.8)#vectorizer_model=vectorizer_model)# Other options for embedding_model are sentence_model, hf_model,roberta_model\n",
        "\n",
        "# Initiate BERTopic\n",
        "topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True,hdbscan_model=kmeans_model, n_gram_range=(1, 3))\n",
        "# Run BERTopic model\n",
        "topics, probabilities = topic_model.fit_transform(papers_nips['abstract_lemmatized'])#abstract_lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dyS2YfzHiW5"
      },
      "outputs": [],
      "source": [
        "# Get the list of topics\n",
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6ri2j26H45U"
      },
      "outputs": [],
      "source": [
        "# Get top 10 terms for a topic\n",
        "topic_model.get_topic(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oRCvcaIIGPN"
      },
      "outputs": [],
      "source": [
        "# Visualize top topic keywords\n",
        "topic_model.visualize_barchart(top_n_topics=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrtbLPEQIUnP"
      },
      "outputs": [],
      "source": [
        "# Visualize term rank decrease\n",
        "topic_model.visualize_term_rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5qFi5v0Ivy4"
      },
      "outputs": [],
      "source": [
        "# Visualize intertopic distance\n",
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h21guvbUI-nU"
      },
      "outputs": [],
      "source": [
        "# Visualize connections between topics using hierachical clustering\n",
        "topic_model.visualize_hierarchy(top_n_topics=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi05gGQ6JLwu"
      },
      "outputs": [],
      "source": [
        "# Visualize similarity using heatmap\n",
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c9_hEHwKICq"
      },
      "outputs": [],
      "source": [
        "# Get the topic predictions\n",
        "topic_prediction = topic_model.topics_[:]\n",
        "# Save the predictions in the dataframe\n",
        "papers_nips['topic_prediction'] = topic_prediction\n",
        "# Take a look at the data\n",
        "papers_nips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrU-6TKSKaB8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# New data for the review\n",
        "new_review = \"I like the new headphone. Its sound quality is great.\"\n",
        "# Find topics\n",
        "num_of_topics = 3\n",
        "similar_topics, similarity = topic_model.find_topics(new_review, top_n=num_of_topics); \n",
        "# Print results\n",
        "print(f'The top {num_of_topics} similar topics are {similar_topics}, and the similarities are {np.round(similarity,2)}')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTxj2YPZKj8T"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Print the top keywords for the top similar topics\n",
        "for i in range(num_of_topics):\n",
        "  print(f'The top keywords for topic {similar_topics[i]} are:')\n",
        "  print(topic_model.get_topic(similar_topics[i]))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZM0Vd87K6cs"
      },
      "outputs": [],
      "source": [
        "# Save the topic model\n",
        "topic_model.save(\"papers_nips_topic_model\")\t\n",
        "# Load the topic model\n",
        "my_model = BERTopic.load(\"papers_nips_topic_model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
