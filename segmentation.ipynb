{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXH1bmUctMld"
      },
      "source": [
        "# Segmentation\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Ali-Alameer/NLP/blob/main/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrHJrKYis06U"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "NLP models often handle different languages with different character sets.  *Unicode* is a standard encoding system that is used to represent characters from almost all languages.  Every Unicode character is encoded using a unique integer [code point](https://en.wikipedia.org/wiki/Code_point) between `0` and `0x10FFFF`. A *Unicode string* is a sequence of zero or more code points.\n",
        "\n",
        "This tutorial shows how to represent Unicode strings in TensorFlow and manipulate them using Unicode equivalents of standard string ops. It separates Unicode strings into tokens based on script detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIKHl5Lvn4gh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx7HEFpBzEsB"
      },
      "source": [
        "## Example: Simple segmentation\n",
        "\n",
        "Segmentation is the task of splitting text into word-like units. This is often easy when space characters are used to separate words, but some languages (like Chinese and Japanese) do not use spaces, and some languages (like German) contain long compounds that must be split in order to analyze their meaning. In web text, different languages and scripts are frequently mixed together, as in \"NY株価\" (New York Stock Exchange).\n",
        "\n",
        "We can perform very rough segmentation (without implementing any ML models) by using changes in script to approximate word boundaries. This will work for strings like the \"NY株価\" example above. It will also work for most languages that use spaces, as the space characters of various scripts are all classified as USCRIPT_COMMON, a special script code that differs from that of any actual text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grsvFiC4BoPb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# dtype: string; shape: [num_sentences]\n",
        "#\n",
        "# The sentences to process.  Edit this line to try out different inputs!\n",
        "sentence_texts = [u'Hello, world.', u'世界こんにちは'] # The 'u' in front of the string values means the string is a Unicode string. Unicode is a way to represent more characters than normal ASCII can manage. The fact that you're seeing the u means you're on Python 2 - strings are Unicode by default on Python 3, but on Python 2, the u in front distinguishes Unicode strings.\n",
        "# try below\n",
        "# sentence_texts = ['A A']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CapnbShuGU8i"
      },
      "source": [
        "First, decode the sentences into character codepoints, and find the script identifeir for each character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReQVcDQh1MB8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]\n",
        "#\n",
        "# sentence_char_codepoint[i, j] is the codepoint for the j'th character in\n",
        "# the i'th sentence.\n",
        "sentence_char_codepoint = tf.strings.unicode_decode(sentence_texts, 'UTF-8')\n",
        "print(sentence_char_codepoint)\n",
        "\n",
        "# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]\n",
        "#\n",
        "# sentence_char_scripts[i, j] is the Unicode script of the j'th character in\n",
        "# the i'th sentence.\n",
        "sentence_char_script = tf.strings.unicode_script(sentence_char_codepoint)\n",
        "print(sentence_char_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "Some relevant examples about code manipulations used below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tf.cancat: Concatenates tensors along one dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "t1 = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = tf.constant([[7, 8, 9], [10, 11, 12]])\n",
        "tf.concat([t1, t2], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tf.fill: Creates a tensor filled with a scalar value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.fill([2, 3], 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.fill([sentence_char_script.nrows(), 1], True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tf.not_equal: Returns the truth value of (x != y) element-wise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sentence_char_script[:, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sentence_char_script[:, :-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tff.squeeze: Removes dimensions of size 1 from the shape of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "t = tf.constant([[[2,3]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.shape(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.shape(tf.squeeze(t)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.squeeze(t).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.shape(tf.squeeze(t, axis=1)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tf.squeeze(t, axis=[1]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2fapF5UGcUc"
      },
      "source": [
        "Use the script identifiers to determine where word boundaries should be added.  Add a word boundary at the beginning of each sentence, and for each character whose script differs from the previous character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v5W6MOr1Rlc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# dtype: bool; shape: [num_sentences, (num_chars_per_sentence)]\n",
        "#\n",
        "# sentence_char_starts_word[i, j] is True if the j'th character in the i'th\n",
        "# sentence is the start of a word. \n",
        "sentence_char_starts_word = tf.concat(\n",
        "    [tf.fill([sentence_char_script.nrows(), 1], True), # the idea is this line is to add a true which indicate word boundry at the begening of each sentence \n",
        "     tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])],\n",
        "    axis=1)\n",
        "\n",
        "# dtype: int64; shape: [num_words]\n",
        "#\n",
        "# word_starts[i] is the index of the character that starts the i'th word (in\n",
        "# the flattened list of characters from all sentences). squeeze return A Tensor. \n",
        "# Has the same type as input. Contains the same data as input, but has one or more \n",
        "# dimensions of size 1 removed. tf.where returns the indices of non-zero elements\n",
        "# axis is an optional list of ints. Defaults to []. If specified, only squeezes the dimensions listed.\n",
        "word_starts = tf.squeeze(tf.where(sentence_char_starts_word.values), axis=1)\n",
        "print(word_starts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAwh-1QkGuC9"
      },
      "source": [
        "You can then use those start offsets to build a `RaggedTensor` containing the list of words from all batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNiA1O_eBBCL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# dtype: int32; shape: [num_words, (num_chars_per_word)]\n",
        "#\n",
        "# word_char_codepoint[i, j] is the codepoint for the j'th character in the\n",
        "# i'th word. the class method from_row_starts creates a RaggedTensor with rows partitioned by row_starts.\n",
        "word_char_codepoint = tf.RaggedTensor.from_row_starts(\n",
        "    values=sentence_char_codepoint.values,\n",
        "    row_starts=word_starts)\n",
        "print(word_char_codepoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66a2ZnYmG2ao"
      },
      "source": [
        "To finish, segment the word codepoints `RaggedTensor` back into sentences and encode into UTF-8 strings for readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCfwcqLSEjZb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# dtype: int64; shape: [num_sentences]\n",
        "#\n",
        "# sentence_num_words[i] is the number of words in the i'th sentence.\n",
        "sentence_num_words = tf.reduce_sum(\n",
        "    tf.cast(sentence_char_starts_word, tf.int64),\n",
        "    axis=1)\n",
        "\n",
        "# dtype: int32; shape: [num_sentences, (num_words_per_sentence), (num_chars_per_word)]\n",
        "#\n",
        "# sentence_word_char_codepoint[i, j, k] is the codepoint for the k'th character\n",
        "# in the j'th word in the i'th sentence.\n",
        "sentence_word_char_codepoint = tf.RaggedTensor.from_row_lengths(\n",
        "    values=word_char_codepoint,\n",
        "    row_lengths=sentence_num_words)\n",
        "print(sentence_word_char_codepoint)\n",
        "\n",
        "tf.strings.unicode_encode(sentence_word_char_codepoint, 'UTF-8').to_list()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oL9KopJirB2g"
      ],
      "name": "unicode.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
