{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NLP Pipeline\n",
    "As we mentioned in the lecture slides, an NLP pipeline is constructed from  the following steps: \n",
    "- Data acquisition, \n",
    "- Text extraction and cleaning \n",
    "- Pre-processing\n",
    "- Feature Engineering\n",
    "- Modelling\n",
    "- Evaluation\n",
    "- Deployement\n",
    "- Monitoring & Model updating\n",
    "\n",
    "In this notebook we will try to explain some of these steps using Pandas,NLTK, String, Contractions and Scikit-learn libraries.  You can open the cloud version of this notebook using the following link:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Ali-Alameer/NLP/blob/main/week2_code.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis\n",
    "With all of the tweets circulating every second, it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in the language is important in these times where decisions and reactions are created and updated in seconds. In this workshop, we'll create an NLP pipeline to predict the sentiment of each tweet.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "\n",
    "In order to do any type of NLP analysis one requires data to analyze. The twitter data can be collected using the twitter API (https://developer.twitter.com/en/docs/twitter-api). Twitter API is the official programmatic endpoint provided by Twitter. It allows developers to access the enormous amount of public data on Twitter that millions of users share daily. \n",
    "\n",
    "Tweepy (https://www.tweepy.org/) is an easy-to-use Python library for accessing the Twitter API. Its API class provides access to the RESTful methods of the Twitter API. We will skip the data acquisition process for this workshop in order to keep it short. However, you can develop the process of extracting tweets from Twitter API as an individual project for your portfolio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "The second step in the NLP pipeline is extracting the text from its native form (such as pdf, image or html files). \n",
    "\n",
    "Our dataset is a CSV(Comma Separated Values) file that contains tweets data. Each row contains the text of a tweet and a sentiment label. We will use the <b>Pandas</b> library to read the CSV file and load data into a dataframe.\n",
    "\n",
    "A <b>Pandas DataFrame</b> is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading CSV files\n",
    "train_raw = pd.read_csv('train_tweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to check the loaded data by displaying the first 5 tweets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of training data\n",
    "train_raw.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how the data is structured, let's take a look at it. There will be a result showing how many rows and columns the dataset contains by printing the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_raw.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id column is not required in our process so we can remove this column. Also, we can rearrange columns in the dataset by brining the tweet text in the first column and a sentiment label in the second column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the columns in the training dataset\n",
    "# and remove the id column\n",
    "train_df = train_raw[['tweet', 'label']]\n",
    "train_df.columns = ['tweet', 'sentiment']\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of positive and negative tweets using the value_counts() method of a dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sentiment.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset description indicates that:\n",
    "- <b>0</b> ==> <b>positive sentiments</b>\n",
    "- <b>1</b> ==> <b>negative sentiments</b>\n",
    "\n",
    "According to the result of the previous cell, there are 29,720 positive tweets and 2,242 negative tweets in the training dataset. As a result, the training dataset is <b>imbalanced</b> since the data points are not equal for the two classes.\n",
    "\n",
    "For storing sentiments, a Python dictionary is an appropriate data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary to map numbers to corresponding sentiments\n",
    "map = {0: 'Positive', 1: 'Negative'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning & pre-processing\n",
    "\n",
    "Why Do We Need to clean and pre-process Text?\n",
    "\n",
    "- <b>Extracting plain text</b>: Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n",
    "- <b>Reducing complexity</b>: Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.\n",
    "\n",
    "\n",
    "In order to clean the text of tweets, we will first create a function that lowercase text, expand contractions, removes text enclosed in square brackets, removes links, removes punctuation, and removes words containing numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import contractions\n",
    "\n",
    "def clean_text(text):\n",
    "    # make text lowercase    \n",
    "    text = str(text).lower()\n",
    "    # expand contractions\n",
    "    text = \" \".join([contractions.fix(expanded_word) for expanded_word in text.split()])\n",
    "    # remove text in square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # remove links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), '', text)\n",
    "    # remove new lines\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean text fuction on each twitte in the training dataset\n",
    "train_df['clean_tweet'] = train_df['tweet'].apply(lambda x:clean_text(x))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Exercise</font>\n",
    "\n",
    "Complete the following code to create a column named \"no_sentences\" containing the number of sentences for each tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# calculate the number of sentences for each tweet\n",
    "# train_df['no_sentences'] = [len(sentences) for sentences in train_df['tweet'].apply(sent_tokenize)]\n",
    "train_df['no_sentences'] = # wirth code here\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization\n",
    "\n",
    "Now we can tokenize tweets into words and extract a list of words for each tweet. We can use the NLTK word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['word_list'] = train_df['clean_tweet'].apply(lambda x:word_tokenize(str(x)))\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most common words in tweets text\n",
    "\n",
    "Before removing stop words it is worth looking at the tweet's word list and extracting the most common words in tweet texts. This step will help us to understand why we need to remove stop words from the word list. \n",
    "\n",
    "In the \"collections\" module of python, you'll find a class specially designed to count several different objects in one go. This class is conveniently called <b>Counter</b>. We use the Counter class to count the number of repetitions of a word in the word list column and then we store the result in a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "top = Counter([item for sublist in train_df['word_list'] for item in sublist])\n",
    "temp_df = pd.DataFrame(top.most_common(20))\n",
    "temp_df.columns = ['Common_words','count']\n",
    "temp_df.style.background_gradient(cmap = 'Blues')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "\n",
    "As you can see, many of the most commonly used words are not useful for identifying tweet sentiment. They belong to the stop words list and should be removed from the tweets words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(word_list):\n",
    "  return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "train_df['word_list_without_sw'] = train_df['word_list'].apply(lambda x:remove_stopword(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's to check the most common words in the tweets after removing all stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in train_df['word_list_without_sw'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp = temp.iloc[1:,:]\n",
    "temp.columns=['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words sentiments wise\n",
    "\n",
    "As a result of this process, we can see some meaningful words among the most common words. As we have more positive tweets in our dataset, positive words have a larger proportion. We can check the most common word in both negative and positive tweets separately. In the following cell, we will create two separate dataframes for each sentiment and repeat the above process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seperate dataframes for each sentiment\n",
    "Positive_sent = train_df[train_df['sentiment'] == 0]\n",
    "Negative_sent = train_df[train_df['sentiment'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MosT common positive words\n",
    "top = Counter([item for sublist in Positive_sent['word_list_without_sw'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(20))\n",
    "temp_positive.columns = ['Common_positive_words','count']\n",
    "temp_positive.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MosT common negative words\n",
    "top = Counter([item for sublist in Negative_sent['word_list_without_sw'] for item in sublist])\n",
    "temp_negative = pd.DataFrame(top.most_common(20))\n",
    "temp_negative = temp_negative.iloc[1:,:]\n",
    "temp_negative.columns = ['Common_negative_words','count']\n",
    "temp_negative.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Both stemming and lemmatization converts word to its base form. Stemming is a fast rule based technique and sometimes chops off inaccurately (under-stemming and over-stemming). You may have noticed NLTK provides PorterStemmer and a slightly improved Snowball Stemmer.\n",
    "\n",
    "Lemmatization is dictionary based technique, more accurate but slightly slower than stemming. We will use WordnetLemmatizer from NLTK. We will download the wordnet resource for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_df['word_list_without_sw'] = train_df['word_list_without_sw'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pre-processing\n",
    "\n",
    "Let's to concatinate all the words in the last column on the dataframe and create a cleaned version of tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['final_tweet'] = train_df['word_list_without_sw'].apply(lambda x:' '.join(x))\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Remember the input data is just a bunch of words for now. Machine learning models only understand and work on numbers. So we have to convert all the text into numbers. In NLP, this conversion of raw text to a suitable numerical form is called <b>text representation</b>. You will learn about this process in the following  lectures. \n",
    "\n",
    "Now that input is clean and ready, we convert it into numbers using something called the <b>\"Bag of Words\"</b> approach. We create a matrix table, where each row represents a sentence and each word will have a separate column for itself that represents its frequency. Let’s take an example, to make it easier. Let’s say there are 3 tweets like: \n",
    "- The food is really bad \n",
    "- The food is really good  \n",
    "- The food is really good and tasty. \n",
    "\n",
    "This could be represented as a matrix using the bag of words approach as shown:\n",
    "\n",
    "| Tweet Number | The | food | is | bad | really | good | tasty | and |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "| 1 |  1 |  1 |  1 |  1 |  1 |  0 |  0 |  0 | \n",
    "| 2 |  1 |  1 |  1 |  0 |  1 |  1 |  0 |  0 | \n",
    "| 3 |  1 |  1 |  1 |  0 |  1 |  1 |  1 |  1 | \n",
    "\n",
    "One problem about this method that you might not notice is that the order of the sentence is lost. There are other approaches to counter this, but we are just going to stick with this method at the moment. You will learn more about other approaches in the following workshops. \n",
    "\n",
    "The CountVectorizer class in the \"sklearn.feature_extraction.text\" module will be utilised to generate Bag of words for our tweets dataset.The Count Vectorizer function converts a list of words into a bag of words, however, notice that we specify something called the max features to it. As you might have seen in the bag of words illustration table, each word will have a separate column. This number of columns can explode into large numbers in big datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 8000, ngram_range = (1,2))\n",
    "X = cv.fit_transform(train_df['final_tweet']).toarray()\n",
    "y = train_df['sentiment']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this we set the max columns as 8000, and keep the maximum occurring 8000 words. Finally, the `cv.fit_transform` function takes the cleaned data and converts it into the bag of words that we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X: \", X[0:5])\n",
    "print(\"y: \",y[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "One of the first decisions to make when starting a modeling phase is how to utilize the existing data. One common technique is to split the data into two groups typically referred to as the `training` and `testing` sets. The training set is used to train a model and the test set is used to evaluate the model. \n",
    "\n",
    "The `train_test_split` class of `sklearn.model_selection` class has been used to generate the training and test set in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"number of tweets in training dataset: \", len(X_train))\n",
    "print(\"number of tweets in testing dataset: \", len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we’ll build classifiers using five well-known machine learning algorithms: \n",
    "- Multinomial Naive Bayes,\n",
    "- Bernoulli Naive Bayes, \n",
    "- logistic regression, \n",
    "- Random Forest, and \n",
    "- Decision Tree \n",
    "\n",
    "You don't need to worry about these models as you will learn detailed implementation of them in the `Machine Learning` module next semester. \n",
    "\n",
    "In the following section, we will import the required libraries to create our classifiers. To evaluate the models after they have been trained, a list of evaluation metrics is also imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an object from each of the classifiers model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "bnb_model = BernoulliNB()\n",
    "lr_model = LogisticRegression(max_iter = 300, multi_class = \"multinomial\")\n",
    "rfc_model = RandomForestClassifier(n_estimators = 50, random_state = 2, max_depth = 25)\n",
    "tree_model = DecisionTreeClassifier(max_depth = 30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the code the following code defined a function that trains and returns the accuracy, and precision scores of the trained model. We also created a dictionary to store our model names. Then using a for loop we will call the function to train and store the socres for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, X_train, y_train, X_test, y_test):\n",
    "    # train the model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate the performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average = \"micro\")\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average = \"micro\")\n",
    "    f1 = f1_score(y_test, y_pred, average = \"micro\")\n",
    "\n",
    "    return accuracy, precision, confusion, recall, f1\n",
    "\n",
    "# create a dictionary of models\n",
    "models = {\n",
    "    'Multinomial Naive Bayes': mnb_model,\n",
    "    'Bernoulli Naive Bayes': bnb_model,\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest Classifier': rfc_model,\n",
    "    'Decision Tree Classifier': tree_model\n",
    "}\n",
    "\n",
    "# create a list to store performance of models\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "confusions = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # train the model on training data\n",
    "    cur_accuracy, cur_precision, cur_confusion, cur_recall, cur_f1 = train_classifier(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # print model performance\n",
    "    print(\"Model: \" , model_name)\n",
    "    print(\"Accuracy: \", cur_accuracy)\n",
    "    print(\"Precision: \", cur_precision)\n",
    "    print(\"Confusion Matrix: \", cur_confusion)\n",
    "    disp = ConfusionMatrixDisplay(cur_confusion, display_labels = map.keys())\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    print(\"Recall: \", cur_recall)\n",
    "    print(\"F1: \", cur_f1)\n",
    "\n",
    "    # append the performance metrics to a list\n",
    "    accuracy_scores.append(cur_accuracy)\n",
    "    precision_scores.append(cur_precision)\n",
    "    confusions.append(cur_confusion)\n",
    "    recall_scores.append(cur_recall)\n",
    "    f1_scores.append(cur_f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "In order to show the performance of all models together a dataframe is created and displayed in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame({'Algorithm': models.keys(), \n",
    "                                'Accuracy': accuracy_scores, \n",
    "                                'Precision': precision_scores, \n",
    "                                'Recall': recall_scores, \n",
    "                                'F1': f1_scores,\n",
    "                                'Confusion': confusions}).sort_values(\"Precision\", ascending = False, ignore_index = True)\n",
    "\n",
    "performance_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all other models, we can see that Logistic regression performs the best with an accuracy score of 95% followed by Multinomail Naive Bayes. As it is a classification task so we will not take recall and precision into consideration because here false-positive and false-negative don’t concern us.\n",
    "\n",
    "Finally, we can save the model using a pickle file so that we can deploy it into the deployment phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"./best_model/LogisticRegression.pickle\"\n",
    "pickle.dump(lr_model, open(file_name, \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86cd70f776a547a286cf4ac2da0f2330b91abb11d3736f6a2d32a6533d7c646d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
